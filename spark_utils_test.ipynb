{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spark  utils test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\".\")\n",
    "from pyspark_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Find kth smallest or largest value in the field of spark DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame( pd.DataFrame([[1, 100],[2, 200],[3, 150],[4, 120],[5, 130],[6, 500],\n",
    "                                                         [7, 400],[8, 110],[9, 100],[10,110] ], columns=['id','value']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----+\n",
      "| id|value|\n",
      "+---+-----+\n",
      "|  9|  100|\n",
      "|  1|  100|\n",
      "| 10|  110|\n",
      "|  8|  110|\n",
      "|  4|  120|\n",
      "|  5|  130|\n",
      "|  3|  150|\n",
      "|  2|  200|\n",
      "|  7|  400|\n",
      "|  6|  500|\n",
      "+---+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(\"value\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('JOB_LOG', ' 3 th position in descending order of value ')\n",
      "('JOB_LOG', 'bin search', 'ix', 'value', 'position')\n",
      "('JOB_LOG', 'bin search', 1, 300.0, 2)\n",
      "('JOB_LOG', 'bin search', 2, 150.0, 4)\n",
      "('JOB_LOG', 'bin search', 3, 225.0, 2)\n",
      "('JOB_LOG', 'bin search', 4, 187.5, 3)\n",
      "('JOB_LOG', 'bin search', 5, 168.75, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_percent_point(df, 'value', 30, maxm=600, order=\"desc\", debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_percent_point(df, 'value', 30, maxm=600, order=\"asc\", debug=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluate a Model given column predictions and labels in spark df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.createDataFrame( pd.DataFrame([[1, 0],[1, 0],[1, 1],[1, 1],[1, 0],[0, 0],\n",
    "                                                         [0, 0],[0, 1],[0, 0],[0,1] ], columns=['label','prediction']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('classes', [1, 0])\n",
      "('Accuracy:', 0.5)\n",
      "   column  precision  recall  f1-score\n",
      "0       1        0.5     0.4  0.444444\n",
      "1       0        0.5     0.6  0.545455\n"
     ]
    }
   ],
   "source": [
    "model_evaluation(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# very fast connected component finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('JOB_LOG', '  ITERATION : ', 5)\n",
      "JOB_LOG total relations count: 7\n",
      "JOB_LOG New pairs found : 2\n",
      "('JOB_LOG', '  ITERATION : ', 4)\n",
      "JOB_LOG total relations count: 9\n",
      "JOB_LOG New pairs found : 4\n",
      "('JOB_LOG', '  ITERATION : ', 3)\n",
      "JOB_LOG total relations count: 5\n",
      "JOB_LOG New pairs found : 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('a', ['c', 'g', 'b']), ('d', ['e', 'f'])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "sc = ss.sparkContext\n",
    "sqlContext =  SQLContext(sc)\n",
    "\n",
    "edges = sc.parallelize([(\"a\",\"b\"),\n",
    "                       (\"b\", \"c\"),\n",
    "                       (\"d\", \"e\"),\n",
    "                       (\"d\", \"f\"),\n",
    "                       (\"g\", \"b\")\n",
    "                       ])\n",
    "res = find_connected_components(sc, edges)\n",
    "res.take(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
